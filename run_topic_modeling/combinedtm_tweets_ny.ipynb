{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/tanz/paper_agri/Agri-venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Select a topic model\n",
    "#model = \"lda\" ; from src.topic_models.lda_model import LDAData, LDAModelTrainer\n",
    "#model = \"prodlda\"; from src.topic_models.prodlda_model_octis import ProdLDAData, ProdLDATrainer\n",
    "model = \"combinedtm\"; from src.topic_models.combinedtm_model import CombinedTMData, CombinedTMTrainer\n",
    "#model = \"bertopic\"; from src.topic_models.bertopic_model import BERTopicData, BERTopicTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a dataset\n",
    "dataset = \"tweets_ny\" # vocab: 5477; remove stopword: 5230\n",
    "preprocessing_params = {\n",
    "    \"vocab_size\": 5000, \n",
    "    \"stopwords\": 'tweets_ny'}\n",
    "\n",
    "# combinedtm & tweets_ny: \n",
    "# 33 seconds per run with cpu only; \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List data path\n",
    "train_path = f\"../data/raw/{dataset}/tokenized_train.csv\"\n",
    "test_path = f\"../data/raw/{dataset}/tokenized_test.csv\"\n",
    "\n",
    "# Set result path\n",
    "result_dir = f\"../data/topic_model_output/{dataset}/{model}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for model training\n",
    "\n",
    "# how many topic words for each topic to be remained, are they printed\n",
    "num_topic_words = 10\n",
    "print_topic_words = True\n",
    "\n",
    "# run n times\n",
    "num_iterations = 1\n",
    "\n",
    "# to get the topic distribution of test data or not\n",
    "run_test = True\n",
    "\n",
    "# parameters_tuning mode or not\n",
    "mode = '' #'parameters_tuning' # ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(50, 200, 0.1, 0.005, 40)]\n"
     ]
    }
   ],
   "source": [
    "# Parameters for topic model, can be revised by go to the definition of \"get_parameter_combinations\"\n",
    "from src.topic_models.utils.data_utils import get_parameter_combinations\n",
    "parameter_combinations = get_parameter_combinations(model)\n",
    "print(parameter_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train texts:   0%|          | 0/4501 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train texts: 100%|██████████| 4501/4501 [00:00<00:00, 143021.58it/s]\n",
      "Batches: 100%|██████████| 23/23 [00:02<00:00,  9.85it/s]\n",
      "Processing test texts: 100%|██████████| 501/501 [00:00<00:00, 133920.48it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  9.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "#lda_data = LDAData(train_path, test_path, preprocessing_params)\n",
    "#prodlda_data = ProdLDAData(train_path, test_path, preprocessing_params)\n",
    "combinedtm_data = CombinedTMData(train_path, test_path, preprocessing_params)\n",
    "#bertopic_data = BERTopicData(train_path, test_path, preprocessing_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 5000\n"
     ]
    }
   ],
   "source": [
    "print(f\"vocabulary size: {len(combinedtm_data.train['vocab'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model trainer\n",
    "#trainer = LDAModelTrainer(lda_data, num_topic_words, parameter_combinations, result_dir, mode=mode, print_topic_words=print_topic_words, run_test=run_test)\n",
    "#trainer = ProdLDATrainer(prodlda_data, num_topic_words, parameter_combinations, result_dir, mode=mode, print_topic_words=print_topic_words, run_test=run_test)\n",
    "trainer = CombinedTMTrainer(combinedtm_data, num_topic_words, parameter_combinations, result_dir, mode=mode, print_topic_words=print_topic_words, run_test=run_test)\n",
    "#trainer = BERTopicTrainer(bertopic_data, num_topic_words, parameter_combinations, result_dir, mode=mode, print_topic_words=print_topic_words, run_test=run_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [40/40]\t Seen Samples: [176000/180040]\tTrain Loss: 53.91614501953125\tTime: 0:00:00.661350: : 40it [00:27,  1.45it/s] \n",
      "100%|██████████| 23/23 [00:00<00:00, 70.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: awesome fun mom promise band sure respect urge happen gossip\n",
      "Topic 1: tweet right book daily live smoke plan tomorrow week supper\n",
      "Topic 2: fatnesstofitness chase florida distance project lame blog research supplement blake\n",
      "Topic 3: learn write wait maybe play remember hour everyday exercise tweet\n",
      "Topic 4: look cursing past apology mentally alcohol share blog involve ultron\n",
      "Topic 5: win look sure care entrepreneur christmas newyearseve fuck gift self\n",
      "Topic 6: pant mentally instagram weather typo simple ass hit selfimprovement follow\n",
      "Topic 7: quit write step shape ignore follow post wear hurt maybe\n",
      "Topic 8: smoke goal tweet drink read right club supper cigarette stay\n",
      "Topic 9: forreal lowcarb value lowcal distance experience economic tagof hash shed\n",
      "Topic 10: unstoppable half screenwriting focus resume jack pretty enjoy body jewelry\n",
      "Topic 11: glass forreal bowl team approval bite growth little awesome girl\n",
      "Topic 12: little awesome chance forward future past false win meet grow\n",
      "Topic 13: right cigarette club supper smoke gym fitness read fit stay\n",
      "Topic 14: come use read wait book quit eve month family live\n",
      "Topic 15: roll pack bucketlist december ready school word follow invest anybody\n",
      "Topic 16: fatnesstofitness lame windbreaker outdoors kindle distance alllllll boomshakalaka amen supplement\n",
      "Topic 17: son resolve grow meet pant win way great future workhard\n",
      "Topic 18: gym tomorrow friend lose weight workout cigarette smoke read book\n",
      "Topic 19: hard run nap write family share hopefully wait personal music\n",
      "Topic 20: use drink instead learn stay know family post continue coffee\n",
      "Topic 21: lame fatnesstofitness windbreaker cary forreal segment laziness speak bake bedroom\n",
      "Topic 22: fit game workout spend kick lose watch gym look today\n",
      "Topic 23: hard way body lol finish smart matter drink addiction cross\n",
      "Topic 24: stay daily gym fat month read week healthy workout tweet\n",
      "Topic 25: right book goal finish club learn supper read smoke cigarette\n",
      "Topic 26: eve cigarette quit friend gym write positive fat smoke watch\n",
      "Topic 27: break white awesome wife smile matter carry boob perfect tix\n",
      "Topic 28: goal lose healthy right gym book week weight smoke tweet\n",
      "Topic 29: meet car marry man wit list gabe cup hilover doubt\n",
      "Topic 30: support script finally story great date drive hit baseball visit\n",
      "Topic 31: gym week goal lose read diet weight workout healthy friend\n",
      "Topic 32: body complete job shit wedding reduce destroy cupcake saturday band\n",
      "Topic 33: argue gossip discover kindle lil respect rich mentally forreal fart\n",
      "Topic 34: sleep place shoot tradition startup app purchase appreciate accomplish hawk\n",
      "Topic 35: tweet workout fitness week lose shape book game hope weight\n",
      "Topic 36: moment edm clean athlete drunk room contact sunday stunt situation\n",
      "Topic 37: right club supper cigarette smoke change goal month read finally\n",
      "Topic 38: organize smile film okay travel nice hurt studio fight generous\n",
      "Topic 39: break pray meet evident sex degree boyfriend lady touch culture\n",
      "Topic 40: twitter gain diet tweet write hard friend daily lose lbs\n",
      "Topic 41: run early tweet workout friend everyday leg social gym weight\n",
      "Topic 42: action comfortable kill care decide water plan excited help real\n",
      "Topic 43: kind christmas car group produce important tape youtube adventure apartment\n",
      "Topic 44: tshirt forreal jersey hear song number quarter send god fatnesstofitness\n",
      "Topic 45: healthy gym diet right workout twitter yoga tweet lose exercise\n",
      "Topic 46: right club supper smoke goal cigarette stay workout week lose\n",
      "Topic 47: sweet fatnesstofitness lame outdoors avoid distance lowcarb attach quarter windbreaker\n",
      "Topic 48: gym lose read right friend week smoke cigarette healthy tweet\n",
      "Topic 49: listen child person iwant dedicate nice white feeling smile regular\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 71.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of train set's topic distribution is (4501, 50)\n",
      "Topic Diversity: 0.41 (used 0.0011 seconds)\n",
      "Inverted Ranked-Biased Overlap: 0.6828836584125126 (used 0.0612 seconds)\n",
      "Topic Coherence u_mass: -17.380975268187207 (used 0.0778 seconds)\n",
      "Topic Coherence c_v: 0.4726533178488593 (used 1.8959 seconds)\n",
      "Topic Coherence c_npmi: -0.3951601955578863 (used 1.9028 seconds)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 13.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of test set's topic distribution is (501, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "trainer.train_and_evaluate(num_iterations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agri-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
