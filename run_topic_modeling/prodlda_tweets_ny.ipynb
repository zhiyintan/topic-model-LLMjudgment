{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/tanz/paper_agri/Agri-venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Select a topic model\n",
    "#model = \"lda\" ; from src.topic_models.lda_model import LDAData, LDAModelTrainer\n",
    "model = \"prodlda\"; from src.topic_models.prodlda_model_octis import ProdLDAData, ProdLDATrainer\n",
    "#model = \"combinedtm\"; from src.topic_models.combinedtm_model import CombinedTMData, CombinedTMTrainer\n",
    "#model = \"bertopic\"; from src.topic_models.bertopic_model import BERTopicData, BERTopicTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a dataset\n",
    "dataset = \"tweets_ny\" # vocab: 5477; remove stopword: 5230\n",
    "preprocessing_params = {\n",
    "    \"vocab_size\": 5000, \n",
    "    \"stopwords\": 'tweets_ny'}\n",
    "\n",
    "# prodlda & tweets_ny: \n",
    "# 13 seconds per run with cpu only; \n",
    "# ? mins per run with gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List data path\n",
    "train_path = f\"../data/raw/{dataset}/tokenized_train.csv\"\n",
    "test_path = f\"../data/raw/{dataset}/tokenized_test.csv\"\n",
    "\n",
    "# Set result path\n",
    "result_dir = f\"../data/topic_model_output/{dataset}/{model}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for model training\n",
    "\n",
    "# how many topic words for each topic to be remained, are they printed\n",
    "num_topic_words = 10\n",
    "print_topic_words = True\n",
    "\n",
    "# run n times\n",
    "num_iterations = 1\n",
    "\n",
    "# to get the topic distribution of test data or not\n",
    "run_test = True\n",
    "\n",
    "# parameters_tuning mode or not\n",
    "mode = '' #'parameters_tuning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100, 200, 0.5, 0.005, 40)]\n"
     ]
    }
   ],
   "source": [
    "# Parameters for topic model, can be revised by go to the definition of \"get_parameter_combinations\"\n",
    "from src.topic_models.utils.data_utils import get_parameter_combinations\n",
    "parameter_combinations = get_parameter_combinations(model)\n",
    "print(parameter_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train texts:   0%|          | 0/4501 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train texts: 100%|██████████| 4501/4501 [00:00<00:00, 141421.99it/s]\n",
      "Processing test texts: 100%|██████████| 501/501 [00:00<00:00, 141238.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "#lda_data = LDAData(train_path, test_path, preprocessing_params)\n",
    "prodlda_data = ProdLDAData(train_path, test_path, preprocessing_params)\n",
    "#combinedtm_data = CombinedTMData(train_path, test_path, preprocessing_params)\n",
    "#bertopic_data = BERTopicData(train_path, test_path, preprocessing_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model trainer\n",
    "#trainer = LDAModelTrainer(lda_data, num_topic_words, parameter_combinations, result_dir, mode=mode, print_topic_words=print_topic_words, run_test=run_test)\n",
    "trainer = ProdLDATrainer(prodlda_data, num_topic_words, parameter_combinations, result_dir, mode=mode, print_topic_words=print_topic_words, run_test=run_test)\n",
    "#trainer = CombinedTMTrainer(combinedtm_data, num_topic_words, parameter_combinations, result_dir, mode=mode, print_topic_words=print_topic_words, run_test=run_test)\n",
    "#trainer = BERTopicTrainer(bertopic_data, num_topic_words, parameter_combinations, result_dir, mode=mode, print_topic_words=print_topic_words, run_test=run_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model with 1 Parameter Combinations:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_topics:100,\n",
      " hidden_size:200,\n",
      " dropout:0.5,\n",
      " learning_rate:0.005,\n",
      " epochs:40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/40]\tSamples: [4501/180040]\tTrain Loss: 99.84006120384706\tTime: 0:00:00.604176\n",
      "Epoch: [1/40]\tSamples: [10/400]\tValidation Loss: 55.351007080078126\tTime: 0:00:00.001707\n",
      "Epoch: [2/40]\tSamples: [9002/180040]\tTrain Loss: 83.99882138821582\tTime: 0:00:00.619563\n",
      "Epoch: [2/40]\tSamples: [10/400]\tValidation Loss: 50.996856689453125\tTime: 0:00:00.001911\n",
      "Epoch: [3/40]\tSamples: [13503/180040]\tTrain Loss: 79.40539214484455\tTime: 0:00:00.492845\n",
      "Epoch: [3/40]\tSamples: [10/400]\tValidation Loss: 52.81776123046875\tTime: 0:00:00.001741\n",
      "Epoch: [4/40]\tSamples: [18004/180040]\tTrain Loss: 77.63797197085543\tTime: 0:00:00.707657\n",
      "Epoch: [4/40]\tSamples: [10/400]\tValidation Loss: 56.07900390625\tTime: 0:00:00.001773\n",
      "Epoch: [5/40]\tSamples: [22505/180040]\tTrain Loss: 77.00076211905167\tTime: 0:00:00.391010\n",
      "Epoch: [5/40]\tSamples: [10/400]\tValidation Loss: 58.04075927734375\tTime: 0:00:00.001475\n",
      "Epoch: [6/40]\tSamples: [27006/180040]\tTrain Loss: 76.9856982194061\tTime: 0:00:00.448671\n",
      "Epoch: [6/40]\tSamples: [10/400]\tValidation Loss: 60.112890625\tTime: 0:00:00.002495\n",
      "Epoch: [7/40]\tSamples: [31507/180040]\tTrain Loss: 76.94189184629978\tTime: 0:00:00.377569\n",
      "Epoch: [7/40]\tSamples: [10/400]\tValidation Loss: 60.596240234375\tTime: 0:00:00.001535\n",
      "Early stopping\n",
      "4501 100\n",
      "Topic 0: mature decide night chip lmfao provide fair surprise push natty\n",
      "Topic 1: appropriate set shxt congress chill wastingtime estate catchingup rough course\n",
      "Topic 2: facebook page mobile pursue fundraiser scrolling medium refuse coincidence chandelier\n",
      "Topic 3: change learn language weave tapestry andwhat month differently goal selfie\n",
      "Topic 4: bitch involve program cursing command twice wrong content visit empire\n",
      "Topic 5: math dance brand cat end americans town pissed vegan nah\n",
      "Topic 6: fat hour probably kind smoke tweet dream early pick plan\n",
      "Topic 7: organic yoga surround friendly school resolve hotyoga lol postive self\n",
      "Topic 8: routine sit cool selfhelp learn idoe belly crochet ooc afraid\n",
      "Topic 9: diet stuff growth mentally rat job hood prove tomorrow active\n",
      "Topic 10: daily football positivity nephews kamp lift slut reddit dew niece\n",
      "Topic 11: quick ensure coast sun manatee break manicure flotrack partake instantly\n",
      "Topic 12: treat christmas daily mountain share render vow multiorgasmic angularjs cheese\n",
      "Topic 13: social medium sportsbiz article account exactly father presence aim insight\n",
      "Topic 14: ago change long evil honest engage cut selfie coffee education\n",
      "Topic 15: lol clean cloud healthy asshole hughe flexx week eating benny\n",
      "Topic 16: finish game hang concert look cut cali favorite age useless\n",
      "Topic 17: month sorry guitar world school idgaf haha kinda bitch differently\n",
      "Topic 18: send crush kill bad gym friend game pred listen waist\n",
      "Topic 19: grow follow know january smile little tweettillihavenomorefriend killin feelsgood stick\n",
      "Topic 20: track banjo quick damnit common wtf read venture relax feedmemore\n",
      "Topic 21: wait thot come positive shape sweat early strip feel ratchet\n",
      "Topic 22: patient school mouth dizzmas bite career booyah boss break photography\n",
      "Topic 23: problem document listen ray society double fuck eccentric finish rose\n",
      "Topic 24: exciting talk chocolate listen super nice nye drunk social half\n",
      "Topic 25: dog courage hit watch dirigible floor fuck record catch responsibility\n",
      "Topic 26: workout slut social stuff emoji rule drive druggie insight medium\n",
      "Topic 27: pepsi gift float picture skin fingernail eating happen unaware tbh\n",
      "Topic 28: today designate cab chariot netflix pull meet driver tran eve\n",
      "Topic 29: improve power internet happen sodium garcia son baseball hold tino\n",
      "Topic 30: store girl attention unplug main feeling big unrush champagne trend\n",
      "Topic 31: big thank bed loser deep seasoned correct fried sexy woman\n",
      "Topic 32: leave kind social joy story productive outrun mind wait piyo\n",
      "Topic 33: neighbor everyday twitter idgaf resolve glass stupidity anyways importantly mixtape\n",
      "Topic 34: play journal pump nervo mills situation rental valentines regret body\n",
      "Topic 35: catchingup eating sorta breakingupwith times yankees square eyeball asf confidently\n",
      "Topic 36: girl expand bringiton direction properties key experienced idea procrastinating materialism\n",
      "Topic 37: big drink write episode ppl mean judge avoid invite care\n",
      "Topic 38: eve pick man tall instagram chicken hangover wish asf crush\n",
      "Topic 39: friend better great criticism neighbor auto college visit bulk speak\n",
      "Topic 40: realize disappear sunday lmfao volunteer heal consulting expose mediate large\n",
      "Topic 41: worry happiness facebook weight hillsong empire ask pray deep everytime\n",
      "Topic 42: fuck guy remember fat jack toilet voice gym break turn\n",
      "Topic 43: little sweet share hope mental effect hit join agent inspiration\n",
      "Topic 44: god real friend fitness rest relationship sleep newyearnewme minus kid\n",
      "Topic 45: badass fitness usual real know pretty mean fatkid beloved friend\n",
      "Topic 46: care hair tweet hard home marathon colleen renewable cursing becomingresponsible\n",
      "Topic 47: allow venture place split carry scratch yucky hope single eddie\n",
      "Topic 48: procrastinating convince church reduce hope scotch emoji curse profanity email\n",
      "Topic 49: laugh fit crunch stay expectation stalk sorta rush kid prevent\n",
      "Topic 50: christmas eve chin great stress sucker finally makeup main nah\n",
      "Topic 51: body cleanse juice awesome ball dance achieve scrolling mobile detox\n",
      "Topic 52: sex gaga idgaf commit blessing faithfully happiness theory homeless alright\n",
      "Topic 53: workout direction pizza rip hashtag bed dedicated sundays stay trivia\n",
      "Topic 54: dick quit sayin week heal ear eventing tix shit leg\n",
      "Topic 55: soda aware sister smokeweed junk everybody campbells coincidence innovativelife heel\n",
      "Topic 56: finish add family publish rly job nothin trip travel begreat\n",
      "Topic 57: way smell bridge rose quiet use pretty steampunk friend miserably\n",
      "Topic 58: gift family excellent alive unicorn hangry wallet eyeball painfully banjo\n",
      "Topic 59: chance instead hard folk woman girl parent use change travel\n",
      "Topic 60: bitch truth little tell mom profile quit fuck today bout\n",
      "Topic 61: clean wrong film fresh monday real faith cookie talk end\n",
      "Topic 62: smell rose count blessing moment breakingupwith mean fashion shit sure\n",
      "Topic 63: hmmm old courage deeply dog wanna machine pass fitness enjoy\n",
      "Topic 64: twitter paunch smoke culture practice open mandatory responsibly drinking resolve\n",
      "Topic 65: goal month crazy painfully pay lol pic pizza situation spend\n",
      "Topic 66: nye foot effort energy poop smile gooooooooooooo fall yaahssellfff plastic\n",
      "Topic 67: garcia culture unstoppable importantly impregnate towel ummm hoe caring excessively\n",
      "Topic 68: smoke pant unpette cigarette liar truth royals nigga sleep productive\n",
      "Topic 69: eve write friend basket lotion fit starbuck close know bae\n",
      "Topic 70: believe niggas beat graduate quit learn hip tomorrow afraid decision\n",
      "Topic 71: drink body little dark star stay vein daily music ground\n",
      "Topic 72: charity thrive loan fundraiser publish drop shoplift baby narrow curve\n",
      "Topic 73: tell health hour gym sodium die right quote roll career\n",
      "Topic 74: ago listen dos thankfully pend old lmao wizard literally flirt\n",
      "Topic 75: guy toilet write inspiration adidas chapter habit emphasis vow rough\n",
      "Topic 76: newyear responsible poorly condescend lmfao weird hot environmentally trivia calendar\n",
      "Topic 77: step game post wait idgaf business organize finally calendar escalator\n",
      "Topic 78: sister brand emojis ensure term accidentally unlimited fine nyc apt\n",
      "Topic 79: cigarette smoke worry supper read club calendar awesome piano lot\n",
      "Topic 80: order water fall lifegoal dance cuz unapologetically kind driver total\n",
      "Topic 81: body speak hit send christmas vegan bad senryu haiku punku\n",
      "Topic 82: early marathon dip mexican treat figure hard awesome teacher burn\n",
      "Topic 83: wise tigers monday august pant positivity class bar color bump\n",
      "Topic 84: live lesson dog min know hungry janggo oomf rest huh\n",
      "Topic 85: wag bark notopant drumkit disappear generic remo fundraiser summerbodie reacquire\n",
      "Topic 86: wait watch pizza newyear weight passion fit draw jaysperations check\n",
      "Topic 87: gradually cha dnt banjo auto memphis behavior simplistic restraint honey\n",
      "Topic 88: drop thank clean come acquire got gain rule german vehicle\n",
      "Topic 89: catchingup publisher reinvent sugary knee shell wake limited frappucino nap\n",
      "Topic 90: eating trust concert garcia left impregnate stuck reflect ummm bite\n",
      "Topic 91: impregnate eating confidently motivate sportsbiz carpe diem santa anne prove\n",
      "Topic 92: garcia abide makeup ummm impregnate order simplistic independent shit august\n",
      "Topic 93: enjoy quit hit fitfam pull holiday remember tweet dedicated little\n",
      "Topic 94: bad cookbook everyday actually literally preparation boot conquer potato dramatically\n",
      "Topic 95: travel tweet eve debt vice read man lemoncello curl goal\n",
      "Topic 96: sad cause modeling wish leagueoflegend risotta athlete act pajama lot\n",
      "Topic 97: discover avoid wine bud aware narrow glass convince wkshp list\n",
      "Topic 98: drink bad white affirmation right tooth hard coffee diem moment\n",
      "Topic 99: run guy great clean gym school matter cuz away twitter\n",
      "\n",
      "\n",
      "The shape of train set's topic distribution is (4501, 100)\n",
      "Topic Diversity: 0.525 (used 0.0012 seconds)\n",
      "Inverted Ranked-Biased Overlap: 0.6837036770509761 (used 0.2377 seconds)\n",
      "Topic Coherence u_mass: -19.3153286734542 (used 0.0793 seconds)\n",
      "Topic Coherence c_v: 0.5107641794715353 (used 3.7316 seconds)\n",
      "Topic Coherence c_npmi: -0.4168687074659708 (used 3.6153 seconds)\n",
      "start to test!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model with 1 Parameter Combinations: 100%|██████████| 1/1 [00:14<00:00, 14.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of test set's topic distribution is (501, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "trainer.train_and_evaluate(num_iterations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agri-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
