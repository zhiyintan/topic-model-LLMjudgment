{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a topic model\n",
    "model = \"lda\" ; from src.topic_models.lda_model import LDAData, LDAModelTrainer\n",
    "#model = \"prodlda\"; from src.topic_models.prodlda_model_octis import ProdLDAData, ProdLDATrainer\n",
    "#model = \"combinedtm\"; from src.topic_models.combinedtm_model import CombinedTMData, CombinedTMTrainer\n",
    "#model = \"bertopic\"; from src.topic_models.bertopic_model import BERTopicData, BERTopicTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a dataset\n",
    "dataset = \"tweets_ny\" # vocab: 5477; remove stopword: 5230\n",
    "preprocessing_params = {\n",
    "    \"vocab_size\": 5000, \n",
    "    \"stopwords\": 'tweets_ny'}\n",
    "\n",
    "# lda & tweets_ny: \n",
    "# 10 seconds per run with cpu only; \n",
    "# ? mins per run with gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List data path\n",
    "train_path = f\"../data/raw/{dataset}/tokenized_train.csv\"\n",
    "test_path = f\"../data/raw/{dataset}/tokenized_test.csv\"\n",
    "\n",
    "# Set result path\n",
    "result_dir = f\"../data/topic_model_output/{dataset}/{model}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for model training\n",
    "\n",
    "# how many topic words for each topic to be remained, are they printed\n",
    "num_topic_words = 10\n",
    "print_topic_words = True\n",
    "\n",
    "# run n times\n",
    "num_iterations = 1\n",
    "\n",
    "# to get the topic distribution of test data or not\n",
    "run_test = True\n",
    "\n",
    "# parameters_tuning mode or not\n",
    "mode = '' #'parameters_tuning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100, 5, 0.01, 0.01, 100, 5000, 0, 0.5, 1, 10, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "# Parameters for topic model, can be revised by go to the definition of \"get_parameter_combinations\"\n",
    "from src.topic_models.utils.data_utils import get_parameter_combinations\n",
    "parameter_combinations = get_parameter_combinations(model)\n",
    "print(parameter_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train texts:   0%|          | 0/4501 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train texts: 100%|██████████| 4501/4501 [00:00<00:00, 68822.25it/s]\n",
      "Processing test texts: 100%|██████████| 501/501 [00:00<00:00, 64075.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "lda_data = LDAData(train_path, test_path, preprocessing_params)\n",
    "#prodlda_data = ProdLDAData(train_path, test_path, preprocessing_params)\n",
    "#combinedtm_data = CombinedTMData(train_path, test_path, preprocessing_params)\n",
    "#bertopic_data = BERTopicData(train_path, test_path, preprocessing_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 5230\n"
     ]
    }
   ],
   "source": [
    "print(f\"vocabulary size: {len(lda_data.train['vocab'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model trainer\n",
    "trainer = LDAModelTrainer(lda_data, num_topic_words, parameter_combinations, result_dir, mode=mode, print_topic_words=print_topic_words, run_test=run_test)\n",
    "#trainer = ProdLDATrainer(prodlda_data, num_topic_words, parameter_combinations, result_dir, mode=mode, print_topic_words=print_topic_words, run_test=run_test)\n",
    "#trainer = CombinedTMTrainer(combinedtm_data, num_topic_words, parameter_combinations, result_dir, mode=mode, print_topic_words=print_topic_words, run_test=run_test)\n",
    "#trainer = BERTopicTrainer(bertopic_data, num_topic_words, parameter_combinations, result_dir, mode=mode, print_topic_words=print_topic_words, run_test=run_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model with 1 Parameter Combinations:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: wanna support city excuse york dslr penny flip rhino photograph\n",
      "Topic 1: lose weight kick excited positive imma thought reading lbs effect\n",
      "Topic 2: hair stuff effort drive pretend peace business high load force\n",
      "Topic 3: watch netflix kill episode friends pet eye season head stack\n",
      "Topic 4: today listen sugar cookie homeless cross rule cheek disappoint low\n",
      "Topic 5: nice school weird neighbor small fix manage express compassion poorly\n",
      "Topic 6: daily lie everybody community party root refer stay dress west\n",
      "Topic 7: drink coffee head kiss ball screw cold pen pot case\n",
      "Topic 8: cotton umm jointhefamily consistent search newjob dont dirt submit bedscroll\n",
      "Topic 9: healthy travel increase inspiration patient chance emotional smoothie physical brown\n",
      "Topic 10: quit fast food lmfao sign star procrastination potato couch cigarette\n",
      "Topic 11: pray account hug somebody blessing twitter engage tennis resolut touch\n",
      "Topic 12: girl kind pack notice beautiful vacation fam material history prob\n",
      "Topic 13: long shoe sock miss embrace paper wash instead hurt mind\n",
      "Topic 14: game food job list feel seriously sad vow suck add\n",
      "Topic 15: finish draw breakfast adult meal intensely novel write excited run\n",
      "Topic 16: thank look morning video routine podcast eve check solidify ultron\n",
      "Topic 17: bad wait super complain attention realize situation jump january pay\n",
      "Topic 18: yesterday app arm expect remind delete button financially false facebook\n",
      "Topic 19: post photo sound upset meat tomorrow goal gunna today yoga\n",
      "Topic 20: smell code soul countdown succeed judgmental document script resoluti wear\n",
      "Topic 21: place boyfriend important store email welcome midnight newsletter unsubscribe kill\n",
      "Topic 22: bacon ground article burger desk real fish organized beyonc obama\n",
      "Topic 23: buy forward dance adventure look consume till master honor honest\n",
      "Topic 24: gym visit achieve grateful quote stupid text tomorrow reply vision\n",
      "Topic 25: live improve moment christ debt bless fruit center quality family\n",
      "Topic 26: learn instead use nyr debate plus cig salad jordan hard\n",
      "Topic 27: pizza matter dog winter glass public parent classic calendar celebrate\n",
      "Topic 28: fucking single song pick drop busy edit properly dip publish\n",
      "Topic 29: workout run bring early wake fall child cardio night fairly\n",
      "Topic 30: past save ask future date begin focused stay alot intake\n",
      "Topic 31: bitch water remove folk dad girlfriend punch tired pop savage\n",
      "Topic 32: close decide bit completely reality release husband hungry popular cake\n",
      "Topic 33: yes night black faith sport mcdonald matter chapter piece value\n",
      "Topic 34: goal eve ass fun set clear mind truly gay meet\n",
      "Topic 35: focus gain avoid fail true january way accomplish come drama\n",
      "Topic 36: man cut share wine short hater pinterest apartment journal reflect\n",
      "Topic 37: smoking quit bang writing march dedicate jan gainz bridge tobacco\n",
      "Topic 38: lot half pant waste wear horrible match fun sneaker fact\n",
      "Topic 39: play end guitar house learn toilet prosper totally video doable\n",
      "Topic 40: world music enjoy advice achievable wedding hope agree extra selfimprovement\n",
      "Topic 41: real fitness online soon negativity learn language movie inspire goal\n",
      "Topic 42: stay actually decision literally goals haha got hell ice track\n",
      "Topic 43: twice career youtube summer position aim february workhard wars week\n",
      "Topic 44: leave diet negative rid add coke min crap vice zero\n",
      "Topic 45: far order foot limit idgaf beginnings hear prayer location abs\n",
      "Topic 46: step simple old attitude voice lord butt difference shut phrase\n",
      "Topic 47: resolve guy stick mom slap cigarettes smoke quit right planning\n",
      "Topic 48: bed baby yoga mile throw roll push air practice dance\n",
      "Topic 49: pound treat create gain way outside project mixtape image record\n",
      "Topic 50: word speak action bae treat able use mix handle relate\n",
      "Topic 51: great white skinny lifestyle blood journey pressure goal plant base\n",
      "Topic 52: photo entire passion buy group save music tbt iphone background\n",
      "Topic 53: meet soda free unplug especially cunt convince internet regular fast\n",
      "Topic 54: little body tomorrow anymore trust attend shoot certification receive male\n",
      "Topic 55: grow blog strong relationship pretty jesus son motto invest deep\n",
      "Topic 56: spend accept burn second nye funny bye way vanilla onlinedate\n",
      "Topic 57: hate build win problem personal rest check habit friendship cause\n",
      "Topic 58: fit stress tiger corner grind goal vegetarian heroin lead blow\n",
      "Topic 59: tweet lbs creative priority goal unstoppable september share writing eventing\n",
      "Topic 60: help hot mouth sweet ppl growth cute win lottery experience\n",
      "Topic 61: challenge hopefully beer choose cuz birthday fear opinion paint overcome\n",
      "Topic 62: hope worth probably happen level self weight spend beard instead\n",
      "Topic 63: promise join die home gym workout membership mentally physically million\n",
      "Topic 64: talk woman cat picture maybe open connected stay way fault\n",
      "Topic 65: social worry drunk medium okay follower bored snapchats ish calm\n",
      "Topic 66: late stand fresh respect famous return bucketlist mess use mountain\n",
      "Topic 67: maybe act idk chew chill band emojis sleepy frequently jam\n",
      "Topic 68: break shit month sit hahaha business spanish asap plane idea\n",
      "Topic 69: clean yeah room film beat special house vegan fitfam sunday\n",
      "Topic 70: positive hour number face away stay chipotle church ugh killing\n",
      "Topic 71: college trip believe heart ticket bout gift loan right way\n",
      "Topic 72: right smoke club cigarette supper dream ready walk sick driver\n",
      "Topic 73: use big care wear damn happen phone present organize school\n",
      "Topic 74: awesome allow crazy comment refuse carry self recipe continue read\n",
      "Topic 75: continue chicken loose skip enjoy feel sister distract slowly fried\n",
      "Topic 76: change complete easy opportunity reach smart shall know task sheet\n",
      "Topic 77: idea wrong chuck month sing cjohnson floor poop web sacrifice\n",
      "Topic 78: hard kid lazy hold graduate consider oop use content degree\n",
      "Topic 79: perfect tonight facebook block getfit eve gym early whatsnext abouttime\n",
      "Topic 80: plan marry motivation set brother studio art hello machine yell\n",
      "Topic 81: figure health fitness favorite weightloss team diet cheer lift leave\n",
      "Topic 82: way mean mind laugh joy body noon look live period\n",
      "Topic 83: follow twitter smile holiday movie season success follower confident definition\n",
      "Topic 84: happiness hurt patience fight meet chicago reason lmao battle know\n",
      "Topic 85: everyday live serve makeup channel pregnant wave choice lack talkin\n",
      "Topic 86: book read high forget fake forgive shave month partner school\n",
      "Topic 87: friend finally tell truth tag beh dwl submit consistent drawsomethe\n",
      "Topic 88: catch box sandal row include chip wear heel ballet jar\n",
      "Topic 89: hit send crush dick candy strive story interesting play heck\n",
      "Topic 90: week fat hashtag guess resolutions human volunteer fly tooth animal\n",
      "Topic 91: turn cool walk boy sex res fuck live everytime football\n",
      "Topic 92: sleep wife marathon swear training prove run regret pure living\n",
      "Topic 93: know god person wish christmas afraid clothe exciting exactly failure\n",
      "Topic 94: exercise sure pay definitely appreciate better responsible practice positivity optimistic\n",
      "Topic 95: hand amwriting raise light write shark dont lovely jail home\n",
      "Topic 96: family hang self weed car progress confidence leave guilt doubt\n",
      "Topic 97: fuck write remember concert newyear settle simply deserve instead different\n",
      "Topic 98: lol come shape pic teach lesson judge motivation total ago\n",
      "Topic 99: nap minute count active instagram power twitter facebook beach commit\n",
      "\n",
      "\n",
      "The shape of train set's topic distribution is (4501, 100)\n",
      "Topic Diversity: 0.78 (used 0.0019 seconds)\n",
      "Inverted Ranked-Biased Overlap: 0.6840121320890046 (used 0.2343 seconds)\n",
      "Topic Coherence u_mass: -16.81623586963225 (used 0.0818 seconds)\n",
      "Topic Coherence c_v: 0.35963196143746373 (used 3.6144 seconds)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model with 1 Parameter Combinations: 100%|██████████| 1/1 [00:11<00:00, 11.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Coherence c_npmi: -0.34338193003371636 (used 3.4214 seconds)\n",
      "The shape of test set's topic distribution is (501, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "trainer.train_and_evaluate(num_iterations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agri-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
