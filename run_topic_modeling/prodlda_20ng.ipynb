{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/tanz/paper_agri/Agri-venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Select a topic model\n",
    "#model = \"lda\" ; from src.topic_models.lda_model import LDAData, LDAModelTrainer\n",
    "model = \"prodlda\"; from src.topic_models.prodlda_model_octis import ProdLDAData, ProdLDATrainer\n",
    "#model = \"combinedtm\"; from src.topic_models.combinedtm_model import CombinedTMData, CombinedTMTrainer\n",
    "#model = \"bertopic\"; from src.topic_models.bertopic_model import BERTopicData, BERTopicTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a dataset\n",
    "dataset = \"20ng\" # vocab: 54437; remove stopwords: 54147\n",
    "preprocessing_params = {\n",
    "    \"vocab_size\": 10000, \n",
    "    \"stopwords\": 'English'}\n",
    "\n",
    "# prodlda & 20ng: \n",
    "# 58 second per run with cpu only; \n",
    "# ? per run with gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List data path\n",
    "train_path = f\"../data/raw/{dataset}/tokenized_train.csv\"\n",
    "test_path = f\"../data/raw/{dataset}/tokenized_test.csv\"\n",
    "\n",
    "# Set result path\n",
    "result_dir = f\"../data/topic_model_output/{dataset}/{model}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for model training\n",
    "\n",
    "# how many topic words for each topic to be remained, are they printed\n",
    "num_topic_words = 10\n",
    "print_topic_words = True\n",
    "\n",
    "# run n times\n",
    "num_iterations = 1\n",
    "\n",
    "# to get the topic distribution of test data or not\n",
    "run_test = True\n",
    "\n",
    "# parameters_tuning mode or not\n",
    "mode = '' #'parameters_tuning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(50, 200, 0.1, 0.001, 40)]\n"
     ]
    }
   ],
   "source": [
    "# Parameters for topic model, can be revised by go to the definition of \"get_parameter_combinations\"\n",
    "from src.topic_models.utils.data_utils import get_parameter_combinations\n",
    "parameter_combinations = get_parameter_combinations(model)\n",
    "print(parameter_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train texts: 100%|██████████| 11314/11314 [00:00<00:00, 11922.45it/s]\n",
      "Processing test texts: 100%|██████████| 7532/7532 [00:00<00:00, 17347.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "#lda_data = LDAData(train_path, test_path, preprocessing_params)\n",
    "prodlda_data = ProdLDAData(train_path, test_path, preprocessing_params)\n",
    "#combinedtm_data = CombinedTMData(train_path, test_path, preprocessing_params)\n",
    "#bertopic_data = BERTopicData(train_path, test_path, preprocessing_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model trainer\n",
    "#trainer = LDAModelTrainer(lda_data, num_topic_words, parameter_combinations, result_dir, mode=mode, print_topic_words=print_topic_words, run_test=run_test)\n",
    "trainer = ProdLDATrainer(prodlda_data, num_topic_words, parameter_combinations, result_dir, mode=mode, print_topic_words=print_topic_words, run_test=run_test)\n",
    "#trainer = CombinedTMTrainer(combinedtm_data, num_topic_words, parameter_combinations, result_dir, mode=mode, print_topic_words=print_topic_words, run_test=run_test)\n",
    "#trainer = BERTopicTrainer(bertopic_data, num_topic_words, parameter_combinations, result_dir, mode=mode, print_topic_words=print_topic_words, run_test=run_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model with 1 Parameter Combinations:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_topics:50,\n",
      " hidden_size:200,\n",
      " dropout:0.1,\n",
      " learning_rate:0.001,\n",
      " epochs:40\n",
      "Epoch: [1/40]\tSamples: [11314/452560]\tTrain Loss: 528.8033045687312\tTime: 0:00:01.846514\n",
      "Epoch: [1/40]\tSamples: [10/400]\tValidation Loss: 264.9925048828125\tTime: 0:00:00.002153\n",
      "Epoch: [2/40]\tSamples: [22628/452560]\tTrain Loss: 486.7578584193256\tTime: 0:00:01.753441\n",
      "Epoch: [2/40]\tSamples: [10/400]\tValidation Loss: 266.592333984375\tTime: 0:00:00.002077\n",
      "Epoch: [3/40]\tSamples: [33942/452560]\tTrain Loss: 476.6321390741007\tTime: 0:00:01.484296\n",
      "Epoch: [3/40]\tSamples: [10/400]\tValidation Loss: 262.7978759765625\tTime: 0:00:00.003657\n",
      "Epoch: [4/40]\tSamples: [45256/452560]\tTrain Loss: 469.4158044996796\tTime: 0:00:01.667344\n",
      "Epoch: [4/40]\tSamples: [10/400]\tValidation Loss: 260.2544189453125\tTime: 0:00:00.002368\n",
      "Epoch: [5/40]\tSamples: [56570/452560]\tTrain Loss: 463.843010629806\tTime: 0:00:01.260526\n",
      "Epoch: [5/40]\tSamples: [10/400]\tValidation Loss: 259.4998779296875\tTime: 0:00:00.002639\n",
      "Epoch: [6/40]\tSamples: [67884/452560]\tTrain Loss: 459.51266941814345\tTime: 0:00:01.308478\n",
      "Epoch: [6/40]\tSamples: [10/400]\tValidation Loss: 258.183544921875\tTime: 0:00:00.002037\n",
      "Epoch: [7/40]\tSamples: [79198/452560]\tTrain Loss: 456.6813630374978\tTime: 0:00:01.254091\n",
      "Epoch: [7/40]\tSamples: [10/400]\tValidation Loss: 251.4596923828125\tTime: 0:00:00.002533\n",
      "Epoch: [8/40]\tSamples: [90512/452560]\tTrain Loss: 455.01660881291986\tTime: 0:00:01.232910\n",
      "Epoch: [8/40]\tSamples: [10/400]\tValidation Loss: 256.57587890625\tTime: 0:00:00.002223\n",
      "Epoch: [9/40]\tSamples: [101826/452560]\tTrain Loss: 452.36315488554004\tTime: 0:00:01.387845\n",
      "Epoch: [9/40]\tSamples: [10/400]\tValidation Loss: 255.730322265625\tTime: 0:00:00.002131\n",
      "Epoch: [10/40]\tSamples: [113140/452560]\tTrain Loss: 450.4487575715099\tTime: 0:00:01.287726\n",
      "Epoch: [10/40]\tSamples: [10/400]\tValidation Loss: 248.366845703125\tTime: 0:00:00.001864\n",
      "Epoch: [11/40]\tSamples: [124454/452560]\tTrain Loss: 448.90531245166386\tTime: 0:00:01.291127\n",
      "Epoch: [11/40]\tSamples: [10/400]\tValidation Loss: 250.03818359375\tTime: 0:00:00.001893\n",
      "Epoch: [12/40]\tSamples: [135768/452560]\tTrain Loss: 447.9871833292823\tTime: 0:00:01.359455\n",
      "Epoch: [12/40]\tSamples: [10/400]\tValidation Loss: 249.7023681640625\tTime: 0:00:00.002568\n",
      "Epoch: [13/40]\tSamples: [147082/452560]\tTrain Loss: 446.33164387042603\tTime: 0:00:01.376890\n",
      "Epoch: [13/40]\tSamples: [10/400]\tValidation Loss: 252.05654296875\tTime: 0:00:00.009385\n",
      "Epoch: [14/40]\tSamples: [158396/452560]\tTrain Loss: 446.0395733093402\tTime: 0:00:01.399506\n",
      "Epoch: [14/40]\tSamples: [10/400]\tValidation Loss: 254.5048095703125\tTime: 0:00:00.002235\n",
      "Epoch: [15/40]\tSamples: [169710/452560]\tTrain Loss: 444.8642156910133\tTime: 0:00:01.336938\n",
      "Epoch: [15/40]\tSamples: [10/400]\tValidation Loss: 253.8314697265625\tTime: 0:00:00.002020\n",
      "Early stopping\n",
      "11314 50\n",
      "Topic 0: team cup toronto watch fan quebec sport chicago vancouver adams\n",
      "Topic 1: use right time like good mean question problem thing state\n",
      "Topic 2: god believe belief question exist claim true christians idea truth\n",
      "Topic 3: svhs numner matting crystalize recommene toasterhead bequidistant itor mtd craftsman\n",
      "Topic 4: disease concern response newsgroup kent easter translator patient forum science\n",
      "Topic 5: numner zipws mtd anthology bequidistant specualtion carlsbad matting recommene archiver\n",
      "Topic 6: svhs matting numner crystalize corporeal toasterhead itor cdq disemination anthology\n",
      "Topic 7: numner sceptic syn probablity inb shadow daresbury corporeal baraff dresden\n",
      "Topic 8: know like time want think tell people thing look come\n",
      "Topic 9: sale monitor offer card printer appreciate screen video meg upgrade\n",
      "Topic 10: thank appreciate advance pointer anybody ini suggestion utility greatly fix\n",
      "Topic 11: sale new offer sell price win shipping player van stevens\n",
      "Topic 12: numner anthology matting crystalize bequidistant dispell mtd toasterhead carlsbad itor\n",
      "Topic 13: year think want work talk cost program people pay care\n",
      "Topic 14: government information state san year new police political francisco law\n",
      "Topic 15: think use bit question people dos mov mean right way\n",
      "Topic 16: key chip use number bit clipper message encrypt algorithm public\n",
      "Topic 17: svhs matting crystalize toasterhead itor cdq disemination orientiation sceptic welbon\n",
      "Topic 18: rear luck chicago somebody toronto clean shop btw expensive hot\n",
      "Topic 19: use file run program support like available ftp need mail\n",
      "Topic 20: god jesus man life day christ thing come world church\n",
      "Topic 21: time good like way year problem cause result think help\n",
      "Topic 22: sexual concentrate behavior delete apply atheists fbi ignorant brain counterexample\n",
      "Topic 23: numner matting svhs mtd antecdent crystalize carlsbad welbon toasterhead dispell\n",
      "Topic 24: engine car ride big mile buy helmet light turn hit\n",
      "Topic 25: thank advance sale printer interested appreciate mail monitor display print\n",
      "Topic 26: team game year play player good point run bad second\n",
      "Topic 27: window use set include software windows graphic file image program\n",
      "Topic 28: numner ulman revisited structures sceptic interection abord corporeal tierra svhs\n",
      "Topic 29: drive problem disk card hard ram motherboard floppy run use\n",
      "Topic 30: people way kill long close man turkish come child massacre\n",
      "Topic 31: window thank file font help windows advance application appreciate graphic\n",
      "Topic 32: uhhh numner mtd carlsbad dispell reasoable corporeal alunatic myronreuger revisited\n",
      "Topic 33: card video memory color mac bus mouse monitor drive problem\n",
      "Topic 34: numner svhs matting dispell infirmity crystalize jaggie toasterhead lamentation probablity\n",
      "Topic 35: numner probablity mtd dispell zipws carlsbad recommene myronreuger jaggie specualtion\n",
      "Topic 36: max chz mmw air mkw nkh urmw mnk evz kir\n",
      "Topic 37: space post mail nasa available datum ames send orbit jpl\n",
      "Topic 38: numner svhs probablity matting crystalize specualtion toasterhead itor mtd inclusive\n",
      "Topic 39: svhs numner matting crystalize toasterhead itor revisited cdq disemination orientiation\n",
      "Topic 40: chip fast speed scsi clipper drive bit car phone buy\n",
      "Topic 41: welbon numner syn mtd runme carlsbad matting myronreuger crystalize programl\n",
      "Topic 42: thank cica advance hello anybody appreciate terminal convert thanx download\n",
      "Topic 43: svhs matting numner crystalize toasterhead itor cdq disemination orientiation dispell\n",
      "Topic 44: gun crime control government case criminal state law country israel\n",
      "Topic 45: israel jews religion peace state claim islam country religious life\n",
      "Topic 46: interested mail paper appreciate internet thank response picture company reply\n",
      "Topic 47: numner specualtion corporeal mtd anthology witrh carlsbad suppositions shackle myronreuger\n",
      "Topic 48: numner probablity mtd carlsbad myronreuger syn inclusive worldtoolkit runme simation\n",
      "Topic 49: msg food sensitivity sex eat effect taste allah reaction flavor\n",
      "\n",
      "\n",
      "The shape of train set's topic distribution is (11314, 50)\n",
      "Topic Diversity: 0.402 (used 0.0034 seconds)\n",
      "Inverted Ranked-Biased Overlap: 0.6593861133245615 (used 0.0666 seconds)\n",
      "Topic Coherence u_mass: -6.749424189268621 (used 1.2590 seconds)\n",
      "Topic Coherence c_v: 0.47026528061295864 (used 9.6327 seconds)\n",
      "Topic Coherence c_npmi: -0.040026059143931564 (used 10.6327 seconds)\n",
      "start to test!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model with 1 Parameter Combinations: 100%|██████████| 1/1 [00:58<00:00, 58.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of test set's topic distribution is (7532, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "trainer.train_and_evaluate(num_iterations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agri-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
