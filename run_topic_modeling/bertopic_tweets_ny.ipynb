{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/tanz/paper_agri/Agri-venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Select a topic model\n",
    "#model = \"lda\" ; from src.topic_models.lda_model import LDAData, LDAModelTrainer\n",
    "#model = \"prodlda\"; from src.topic_models.prodlda_model_octis import ProdLDAData, ProdLDATrainer\n",
    "#model = \"combinedtm\"; from src.topic_models.combinedtm_model import CombinedTMData, CombinedTMTrainer\n",
    "model = \"bertopic\"; from src.topic_models.bertopic_model import BERTopicData, BERTopicTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a dataset\n",
    "dataset = \"tweets_ny\" # vocab: 5477; remove stopword: 5230\n",
    "preprocessing_params = {\n",
    "    \"vocab_size\": 5000, \n",
    "    \"stopwords\": 'tweets_ny'}\n",
    "\n",
    "# BERTopic & tweets_ny: \n",
    "# ? seconds per run with cpu only; \n",
    "# 15 seconds per run with gpu;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List data path\n",
    "train_path = f\"../data/raw/{dataset}/tokenized_train.csv\"\n",
    "test_path = f\"../data/raw/{dataset}/tokenized_test.csv\"\n",
    "\n",
    "# Set result path\n",
    "result_dir = f\"../data/topic_model_output/{dataset}/{model}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for model training\n",
    "\n",
    "# how many topic words for each topic to be remained, are they printed\n",
    "num_topic_words = 10\n",
    "print_topic_words = True\n",
    "\n",
    "# run n times\n",
    "num_iterations = 1\n",
    "\n",
    "# to get the topic distribution of test data or not\n",
    "run_test = True\n",
    "\n",
    "# parameters_tuning mode or not\n",
    "mode = '' #'parameters_tuning' # ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(25, 20, 20, 0.3)]\n"
     ]
    }
   ],
   "source": [
    "# Parameters for topic model, can be revised by go to the definition of \"get_parameter_combinations\"\n",
    "from src.topic_models.utils.data_utils import get_parameter_combinations\n",
    "parameter_combinations = get_parameter_combinations(model)\n",
    "print(parameter_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train texts:   0%|                                                                           | 0/4501 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train texts: 100%|█████████████████████████████████████████████████████████████| 4501/4501 [00:00<00:00, 69602.24it/s]\n",
      "Processing test texts: 100%|████████████████████████████████████████████████████████████████| 501/501 [00:00<00:00, 67506.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "#lda_data = LDAData(train_path, test_path, preprocessing_params)\n",
    "#prodlda_data = ProdLDAData(train_path, test_path, preprocessing_params)\n",
    "#combinedtm_data = CombinedTMData(train_path, test_path, preprocessing_params)\n",
    "bertopic_data = BERTopicData(train_path, test_path, preprocessing_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 5000\n"
     ]
    }
   ],
   "source": [
    "print(f\"vocabulary size: {len(bertopic_data.train['vocab'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model trainer\n",
    "#trainer = LDAModelTrainer(lda_data, num_topic_words, parameter_combinations, result_dir, mode=mode, print_topic_words=print_topic_words, run_test=run_test)\n",
    "#trainer = ProdLDATrainer(prodlda_data, num_topic_words, parameter_combinations, result_dir, mode=mode, print_topic_words=print_topic_words, run_test=run_test)\n",
    "#trainer = CombinedTMTrainer(combinedtm_data, num_topic_words, parameter_combinations, result_dir, mode=mode, print_topic_words=print_topic_words, run_test=run_test)\n",
    "trainer = BERTopicTrainer(bertopic_data, num_topic_words, parameter_combinations, result_dir, mode=mode, print_topic_words=print_topic_words, run_test=run_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model with 1 Parameter Combinations:   0%|                                                        | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████████████████████████████████████████████████| 141/141 [00:01<00:00, 102.25it/s]\n",
      "2025-04-10 14:22:14,941 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-04-10 14:22:15,429 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-10 14:22:15,432 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-04-10 14:22:15,563 - BERTopic - Cluster - Completed ✓\n",
      "2025-04-10 14:22:15,568 - BERTopic - Representation - Extracting topics from clusters using representation models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func: all_points_membership_vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 14:22:16,249 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_words:\t520\n",
      "Topic -1: 10, learn yoga wanna live fit shape quit job promise fat\n",
      "Topic 0: 10, dot dick joke nut dramatic phat bmv type liberal repeat\n",
      "Topic 1: 10, drink coffee alcohol soda coke whiskey starbucks detox addiction latte\n",
      "Topic 2: 10, tweet twitter instagram facebook gossip socialmedia unfollow phone platform addicted\n",
      "Topic 3: 10, food pizza chicken taco chipotle meal breakfast recipe gluten mcdonald\n",
      "Topic 4: 10, workout fitness join treadmill fitbit gymlife golf motivation runchat fitnessmotivation\n",
      "Topic 5: 10, music listen concert dance mixtape band soundtrack sing musician drum\n",
      "Topic 6: 10, blog netflix mondayblog podcast rachelclarkgiveaway blogge blogging simpson facebook friends\n",
      "Topic 7: 10, success plan goals step obstacle task achievable realistic hesitation streak\n",
      "Topic 8: 10, sleep nap morning noon sunrise hibernate routine habit snooze evening\n",
      "Topic 9: 10, gainz diet fat pretzel loseweight healthy tweak dropthebabyweight twinkie shed\n",
      "Topic 10: 10, god pray jesus prayer faith bible bless miracle church hug\n",
      "Topic 11: 10, christmas clean room organize apartment stuff folder gift roommate loveyourlife\n",
      "Topic 12: 10, january newyearnewme calendar newyearseve birthday cyclical sunday jesusyear jules gregorian\n",
      "Topic 13: 10, game watch laker hockey playoff yankees cheer favorite fuel teamless\n",
      "Topic 14: 10, selfie portrait evar ordinarylife foley shatter disposable editing nude jim\n",
      "Topic 15: 10, thank pretty bath sexymonday nana enjoy cute beautiful grateful heart\n",
      "Topic 16: 10, chapter publisher challenge thrones amreade novel bible cupid newauthor godslovechat\n",
      "Topic 17: 10, pack shoe sock sneaker fashion legging dress wardrobe underwear sweat\n",
      "Topic 18: 10, hard lie honest truth govenment fascist secret propaganda confession adventuresinchemistry\n",
      "Topic 19: 10, forgive mistake forget blame excuse apology forgiveness regret guilty unpette\n",
      "Topic 20: 10, family father mom resolutions sheep taxpayer mombiz sonoma hangry bournemouth\n",
      "Topic 21: 10, travel adventure boat vacation trip flight ride york iceland hike\n",
      "Topic 22: 10, friend friendship sister needafriend london dwl friendzone sweetie pony relationships\n",
      "Topic 23: 10, bitch dick feminist motherfucker timb rapper bitchez badder hate rape\n",
      "Topic 24: 10, meet justin humble battle chicago marry plato singlelife natasha datingadvice\n",
      "Topic 25: 10, chuck jack blair cjohnson lou waldorf beyonc buck irish wwkkd\n",
      "Topic 26: 10, degree graduate internship recruit teacher career gradua recommendation greensboro engineer\n",
      "Topic 27: 10, spend debt budget finance account wallet student archive bank goodwill\n",
      "Topic 28: 10, perf nutshell troll fuckboi heckle commas don license driver bullshit\n",
      "Topic 29: 10, cosplay strokin title terminator genisys goddess cosby checkoutmyvine pancake batwoman\n",
      "Topic 30: 10, club smoke cigarettes stank pete factz hun partying cancer dope\n",
      "Topic 31: 10, hair shave eyebrow beard eyeliner haircut effort mustache lara trim\n",
      "Topic 32: 10, follow boyrusher insurgent goingstrong frankie fuckboy coach stuck flip rule\n",
      "Topic 33: 10, run mile marathon runnerspace racing wyoming marathontraine marathontraining educator iowa\n",
      "Topic 34: 10, worry fear anxiety stress cookie panic pandora claustrophobic caution happyheart\n",
      "Topic 35: 10, write memory pen writing memo mathchat javascript ink erase undergrad\n",
      "Topic 36: 10, focused stress thoughtless stillquiethour endoftheyear stillness greatness happier comfort vision\n",
      "Topic 37: 10, change anarchy ariel realworld ooc general justice collective mike wave\n",
      "Topic 38: 10, hashtag hashtagoftheweek smoking organize hashtagofthe hammer medical sew recycle broad\n",
      "Topic 39: 10, happiness positivity pessimistic immortality affect unhappy constructive emotion patient motivate\n",
      "Topic 40: 10, smile ride stress symptom frown quitsmoke wink obstacle period motivated\n",
      "Topic 41: 10, car wheel turbo vehicle passenger porsche newdriver lexus wrangler frog\n",
      "Topic 42: 10, weed smoke quit smokin cig smokefree smokeweed firework cannabis tobacco\n",
      "Topic 43: 10, hate hater ugly reason cat losin adorable grumpy disneyland nyr\n",
      "Topic 44: 10, complete cat absolutely lyfe shb flawless accent british content ready\n",
      "Topic 45: 10, girlfriend wife bae impregnate marriage speciman starbswiththegirl virginity slutty wedding\n",
      "Topic 46: 10, healthy health insurance plaindealer healthyteeth uccessful legit physique giant cholesterol\n",
      "Topic 47: 10, live connected active teachit lovelife rank livelifehappy liveit leslie prosper\n",
      "Topic 48: 10, break reality watch vain destroyallcliche afternoon musical lost coast addiction\n",
      "Topic 49: 10, procrastinate procrastination late procrastinating deadline hour sketch dreamer rush workhard\n",
      "Topic 50: 10, care community service volunteer organization contribute generous society compassion family\n",
      "\n",
      "\n",
      "The shape of train set's topic distribution is (4501, 51)\n",
      "Topic Diversity: 0.9326923076923077 (used 0.0029 seconds)\n",
      "Inverted Ranked-Biased Overlap: 0.6949918980570137 (used 0.0710 seconds)\n",
      "Topic Coherence u_mass: -18.087768764707118 (used 0.0772 seconds)\n",
      "Topic Coherence c_v: 0.4178565213730877 (used 3.6887 seconds)\n",
      "Topic Coherence c_npmi: -0.31344304280157054 (used 3.9667 seconds)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 247.13it/s]\n",
      "2025-04-10 14:22:24,220 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-04-10 14:22:24,226 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-10 14:22:24,227 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-10 14:22:24,229 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
      "2025-04-10 14:22:24,231 - BERTopic - Probabilities - Completed ✓\n",
      "2025-04-10 14:22:24,232 - BERTopic - Cluster - Completed ✓\n",
      "Training Model with 1 Parameter Combinations: 100%|████████████████████████████████████████████████| 1/1 [00:14<00:00, 14.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func: approximate_predict\n",
      "func: membership_vector\n",
      "The shape of test set's topic distribution is (501, 51)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "trainer.train_and_evaluate(num_iterations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
